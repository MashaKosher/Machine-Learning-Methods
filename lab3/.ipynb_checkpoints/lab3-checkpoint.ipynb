{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Лабораторная работа 3: Генерация датасета для регрессии\n",
        "\n",
        "В этой лабораторной работе мы создадим синтетический датасет для задачи регрессии с использованием `sklearn.datasets.make_regression` и выполним необходимое масштабирование данных.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Импорт необходимых библиотек\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Настройка стиля графиков\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Генерация синтетического датасета для регрессии\n",
        "\n",
        "Создадим датасет с помощью `make_regression` с следующими параметрами:\n",
        "- **n_samples**: 1000 - количество образцов\n",
        "- **n_features**: 5 - количество признаков (больше минимальных 2)\n",
        "- **n_informative**: 3 - количество информативных признаков  \n",
        "- **noise**: 10 - уровень шума\n",
        "- **random_state**: 42 - для воспроизводимости результатов\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Генерация датасета для регрессии\n",
        "X, y = make_regression(\n",
        "    n_samples=1000,      # количество образцов\n",
        "    n_features=5,        # количество признаков (больше минимальных 2)\n",
        "    n_informative=3,     # количество информативных признаков\n",
        "    n_redundant=1,       # количество избыточных признаков\n",
        "    noise=10,            # уровень шума\n",
        "    random_state=42      # для воспроизводимости\n",
        ")\n",
        "\n",
        "print(\"Форма матрицы признаков X:\", X.shape)\n",
        "print(\"Форма вектора целевых значений y:\", y.shape)\n",
        "print(\"\\nИнформация о данных:\")\n",
        "print(\"Количество образцов:\", X.shape[0])\n",
        "print(\"Количество признаков:\", X.shape[1])\n",
        "print(\"Количество весов (включая bias):\", X.shape[1] + 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Создание DataFrame для удобства работы\n",
        "feature_names = [f'feature_{i+1}' for i in range(X.shape[1])]\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "df['target'] = y\n",
        "\n",
        "print(\"Первые 5 строк датасета:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nСтатистическое описание:\")\n",
        "print(df.describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Анализ необходимости масштабирования данных\n",
        "\n",
        "Проверим распределение значений признаков и целевой переменной, чтобы определить необходимость масштабирования.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Анализ масштаба признаков\n",
        "print(\"Диапазоны значений признаков:\")\n",
        "for col in feature_names:\n",
        "    print(f\"{col}: [{df[col].min():.2f}, {df[col].max():.2f}], std: {df[col].std():.2f}\")\n",
        "\n",
        "print(f\"\\nЦелевая переменная: [{df['target'].min():.2f}, {df['target'].max():.2f}], std: {df['target'].std():.2f}\")\n",
        "\n",
        "# Проверка необходимости масштабирования\n",
        "feature_ranges = [df[col].max() - df[col].min() for col in feature_names]\n",
        "max_range = max(feature_ranges)\n",
        "min_range = min(feature_ranges)\n",
        "range_ratio = max_range / min_range\n",
        "\n",
        "print(f\"\\nОтношение максимального к минимальному диапазону: {range_ratio:.2f}\")\n",
        "if range_ratio > 10:\n",
        "    print(\"⚠️  Рекомендуется масштабирование - большая разница в диапазонах признаков\")\n",
        "else:\n",
        "    print(\"✅ Масштабирование не критично, но может улучшить производительность алгоритмов\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Применение масштабирования данных\n",
        "\n",
        "Применим два популярных метода масштабирования:\n",
        "1. **StandardScaler** (стандартизация) - приводит данные к нулевому среднему и единичной дисперсии\n",
        "2. **MinMaxScaler** (нормализация) - масштабирует данные в диапазон [0, 1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Разделение на обучающую и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Размеры выборок:\")\n",
        "print(f\"Обучающая выборка: X_train {X_train.shape}, y_train {y_train.shape}\")\n",
        "print(f\"Тестовая выборка: X_test {X_test.shape}, y_test {y_test.shape}\")\n",
        "\n",
        "# 1. Стандартизация (StandardScaler)\n",
        "scaler_standard = StandardScaler()\n",
        "X_train_standard = scaler_standard.fit_transform(X_train)\n",
        "X_test_standard = scaler_standard.transform(X_test)\n",
        "\n",
        "# 2. Нормализация (MinMaxScaler)\n",
        "scaler_minmax = MinMaxScaler()\n",
        "X_train_minmax = scaler_minmax.fit_transform(X_train)\n",
        "X_test_minmax = scaler_minmax.transform(X_test)\n",
        "\n",
        "print(\"\\n✅ Масштабирование выполнено успешно!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Сравнение статистик до и после масштабирования\n",
        "print(\"Сравнение статистик признаков:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Оригинальные данные\n",
        "print(\"ОРИГИНАЛЬНЫЕ ДАННЫЕ:\")\n",
        "print(f\"Среднее: {X_train.mean(axis=0)}\")\n",
        "print(f\"Стд. отклонение: {X_train.std(axis=0)}\")\n",
        "print(f\"Минимум: {X_train.min(axis=0)}\")\n",
        "print(f\"Максимум: {X_train.max(axis=0)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# После стандартизации\n",
        "print(\"ПОСЛЕ СТАНДАРТИЗАЦИИ (StandardScaler):\")\n",
        "print(f\"Среднее: {X_train_standard.mean(axis=0)}\")\n",
        "print(f\"Стд. отклонение: {X_train_standard.std(axis=0)}\")\n",
        "print(f\"Минимум: {X_train_standard.min(axis=0)}\")\n",
        "print(f\"Максимум: {X_train_standard.max(axis=0)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# После нормализации\n",
        "print(\"ПОСЛЕ НОРМАЛИЗАЦИИ (MinMaxScaler):\")\n",
        "print(f\"Среднее: {X_train_minmax.mean(axis=0)}\")\n",
        "print(f\"Стд. отклонение: {X_train_minmax.std(axis=0)}\")\n",
        "print(f\"Минимум: {X_train_minmax.min(axis=0)}\")\n",
        "print(f\"Максимум: {X_train_minmax.max(axis=0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Визуализация датасета\n",
        "\n",
        "Создадим графики для анализа сгенерированного датасета и эффектов масштабирования.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Распределение признаков и целевой переменной\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "fig.suptitle('Распределение признаков и целевой переменной', fontsize=16)\n",
        "\n",
        "# Распределение признаков\n",
        "for i, col in enumerate(feature_names):\n",
        "    if i < 5:  # показываем первые 5 признаков\n",
        "        axes[i//3, i%3].hist(df[col], bins=30, alpha=0.7, color=f'C{i}')\n",
        "        axes[i//3, i%3].set_title(f'{col}')\n",
        "        axes[i//3, i%3].set_xlabel('Значение')\n",
        "        axes[i//3, i%3].set_ylabel('Частота')\n",
        "\n",
        "# Распределение целевой переменной\n",
        "axes[1, 2].hist(df['target'], bins=30, alpha=0.7, color='red')\n",
        "axes[1, 2].set_title('target')\n",
        "axes[1, 2].set_xlabel('Значение')\n",
        "axes[1, 2].set_ylabel('Частота')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Корреляционная матрица\n",
        "plt.figure(figsize=(10, 8))\n",
        "correlation_matrix = df.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
        "            square=True, fmt='.2f')\n",
        "plt.title('Корреляционная матрица признаков и целевой переменной')\n",
        "plt.show()\n",
        "\n",
        "# Выводим наиболее коррелированные с целевой переменной признаки\n",
        "target_correlations = correlation_matrix['target'].abs().sort_values(ascending=False)\n",
        "print(\"Корреляция признаков с целевой переменной:\")\n",
        "for feature, corr in target_correlations.items():\n",
        "    if feature != 'target':\n",
        "        print(f\"{feature}: {corr:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Scatter plots наиболее информативных признаков с целевой переменной\n",
        "# Выберем 3 наиболее коррелированных признака\n",
        "top_features = target_correlations[1:4].index.tolist()  # исключаем саму target\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "fig.suptitle('Зависимость целевой переменной от наиболее информативных признаков', fontsize=14)\n",
        "\n",
        "for i, feature in enumerate(top_features):\n",
        "    axes[i].scatter(df[feature], df['target'], alpha=0.6, s=20)\n",
        "    axes[i].set_xlabel(feature)\n",
        "    axes[i].set_ylabel('target')\n",
        "    axes[i].set_title(f'{feature} vs target (r={target_correlations[feature]:.3f})')\n",
        "    \n",
        "    # Добавим линию тренда\n",
        "    z = np.polyfit(df[feature], df['target'], 1)\n",
        "    p = np.poly1d(z)\n",
        "    axes[i].plot(df[feature], p(df[feature]), \"r--\", alpha=0.8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Сравнение эффектов масштабирования\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# Выберем первый признак для демонстрации\n",
        "feature_idx = 0\n",
        "feature_name = feature_names[feature_idx]\n",
        "\n",
        "# Оригинальные данные\n",
        "axes[0].hist(X_train[:, feature_idx], bins=30, alpha=0.7, color='blue', label='Обучающая')\n",
        "axes[0].hist(X_test[:, feature_idx], bins=30, alpha=0.5, color='red', label='Тестовая')\n",
        "axes[0].set_title(f'Оригинальные данные\\n{feature_name}')\n",
        "axes[0].set_xlabel('Значение')\n",
        "axes[0].set_ylabel('Частота')\n",
        "axes[0].legend()\n",
        "\n",
        "# Стандартизированные данные\n",
        "axes[1].hist(X_train_standard[:, feature_idx], bins=30, alpha=0.7, color='blue', label='Обучающая')\n",
        "axes[1].hist(X_test_standard[:, feature_idx], bins=30, alpha=0.5, color='red', label='Тестовая')\n",
        "axes[1].set_title(f'После стандартизации\\n{feature_name}')\n",
        "axes[1].set_xlabel('Значение')\n",
        "axes[1].set_ylabel('Частота')\n",
        "axes[1].legend()\n",
        "\n",
        "# Нормализованные данные\n",
        "axes[2].hist(X_train_minmax[:, feature_idx], bins=30, alpha=0.7, color='blue', label='Обучающая')\n",
        "axes[2].hist(X_test_minmax[:, feature_idx], bins=30, alpha=0.5, color='red', label='Тестовая')\n",
        "axes[2].set_title(f'После нормализации\\n{feature_name}')\n",
        "axes[2].set_xlabel('Значение')\n",
        "axes[2].set_ylabel('Частота')\n",
        "axes[2].legend()\n",
        "\n",
        "plt.suptitle('Сравнение методов масштабирования данных', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Сохранение данных\n",
        "\n",
        "Сохраним обработанные данные для дальнейшего использования в задачах машинного обучения.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Создание итогового DataFrame с различными версиями данных\n",
        "datasets = {\n",
        "    'original': {\n",
        "        'X_train': X_train,\n",
        "        'X_test': X_test,\n",
        "        'y_train': y_train,\n",
        "        'y_test': y_test\n",
        "    },\n",
        "    'standardized': {\n",
        "        'X_train': X_train_standard,\n",
        "        'X_test': X_test_standard,\n",
        "        'y_train': y_train,\n",
        "        'y_test': y_test\n",
        "    },\n",
        "    'normalized': {\n",
        "        'X_train': X_train_minmax,\n",
        "        'X_test': X_test_minmax,\n",
        "        'y_train': y_train,\n",
        "        'y_test': y_test\n",
        "    }\n",
        "}\n",
        "\n",
        "# Сохранение в CSV для дальнейшего использования\n",
        "original_df = pd.DataFrame(X_train, columns=feature_names)\n",
        "original_df['target'] = y_train\n",
        "original_df.to_csv('train_data_original.csv', index=False)\n",
        "\n",
        "print(\"Данные успешно сгенерированы и обработаны!\")\n",
        "print(f\"Сохранены обучающие данные в файл: train_data_original.csv\")\n",
        "print(\"\\nДоступные варианты датасета:\")\n",
        "for key in datasets.keys():\n",
        "    print(f\"- {key}: готов для использования в переменных X_train_{key.replace('original', '').replace('ized', '').replace('ed', '')}\")\n",
        "\n",
        "print(f\"\\nИтоговая информация о датасете:\")\n",
        "print(f\"Количество образцов: {X.shape[0]}\")\n",
        "print(f\"Количество признаков: {X.shape[1]}\")\n",
        "print(f\"Количество весов модели (с bias): {X.shape[1] + 1}\")\n",
        "print(f\"Размер обучающей выборки: {X_train.shape[0]}\")\n",
        "print(f\"Размер тестовой выборки: {X_test.shape[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Заключение\n",
        "\n",
        "В данной лабораторной работе мы успешно:\n",
        "\n",
        "1. **Сгенерировали синтетический датасет** с помощью `sklearn.datasets.make_regression`:\n",
        "   - 1000 образцов\n",
        "   - 5 признаков (больше минимальных 2)\n",
        "   - 3 информативных признака\n",
        "   - 6 весов в модели (включая bias)\n",
        "\n",
        "2. **Проанализировали данные**:\n",
        "   - Изучили распределения признаков и целевой переменной\n",
        "   - Построили корреляционную матрицу\n",
        "   - Определили наиболее информативные признаки\n",
        "\n",
        "3. **Применили масштабирование данных**:\n",
        "   - StandardScaler (стандартизация) - среднее = 0, стд. отклонение = 1\n",
        "   - MinMaxScaler (нормализация) - диапазон [0, 1]\n",
        "\n",
        "4. **Визуализировали результаты**:\n",
        "   - Распределения признаков\n",
        "   - Корреляции с целевой переменной\n",
        "   - Эффекты масштабирования\n",
        "\n",
        "Датасет готов для использования в задачах машинного обучения!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "t"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
