# Лабораторная работа 3: Генерация датасета для регрессии и градиентный спуск

## Описание

Данная лабораторная работа посвящена генерации синтетического датасета для задач регрессии с использованием библиотеки scikit-learn, применению различных методов масштабирования данных и реализации алгоритмов градиентного спуска.

## Структура notebook'а

### 1. Импорт необходимых библиотек

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import make_regression
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
```

**Что делается:**
- Импортируются все необходимые библиотеки для работы с данными, визуализации и машинного обучения
- Выводятся версии используемых библиотек для воспроизводимости

### 2. Генерация синтетического датасета для регрессии

**Параметры `make_regression`:**
- `n_samples=1000` - количество образцов в датасете
- `n_features=5` - количество признаков (больше минимальных 2, как требовалось)
- `n_informative=3` - количество информативных признаков
- `noise=10.0` - уровень шума в данных
- `bias=100.0` - смещение для более реалистичного датасета
- `random_state=42` - для воспроизводимости результатов

**Результат:**
- Создается датасет с 5 признаками и 6 весами модели (включая bias)
- Выводится статистическая информация о сгенерированных данных

### 3. Анализ необходимости масштабирования данных

**Что анализируется:**
- Диапазоны значений каждого признака
- Стандартные отклонения признаков
- Отношение максимального к минимальному диапазону
- Отношение максимального к минимальному стандартному отклонению

**Критерии принятия решения:**
- Отношение > 10 → **обязательно** масштабирование
- Отношение > 3 → **желательно** масштабирование
- Отношение < 3 → масштабирование не критично

### 4. Применение масштабирования данных

**Последовательность действий:**
1. **Разделение данных** на обучающую (80%) и тестовую (20%) выборки
2. **StandardScaler** - стандартизация (μ=0, σ=1)
3. **MinMaxScaler** - нормализация в диапазон [0,1]

**Важно:** Масштабирование применяется после разделения данных для избежания утечки данных (data leakage).

### 5. Визуализация и анализ данных

**Типы графиков:**
- **Гистограммы распределений** всех признаков и целевой переменной
- **Корреляционная матрица** с цветовым кодированием силы связей
- **Scatter plots** наиболее информативных признаков с линиями тренда
- **Сравнение эффектов масштабирования** до и после применения

### 6. Сохранение обработанных данных

**Создаются файлы:**
- `train_data_original.csv` - оригинальные обучающие данные
- `test_data_original.csv` - оригинальные тестовые данные
- `train_data_standardized.csv` - стандартизированные обучающие данные
- `test_data_standardized.csv` - стандартизированные тестовые данные
- `train_data_normalized.csv` - нормализованные обучающие данные
- `test_data_normalized.csv` - нормализованные тестовые данные

### 7. Реализация градиентного спуска

**Функция `gradient_descent`:**
- Инициализация нулевых весов
- Добавление столбца единиц для bias
- Итеративное обновление весов по формуле: `w = w - α∇J(w)`
- Возвращает найденные оптимальные веса

**Применение:**
- Обучение на стандартизированных данных
- Вычисление среднеквадратичной ошибки на тестовой выборке

### 8. Эксперименты со скоростью обучения

**Тестируемые значения:** [0.001, 0.01, 0.1, 1.0, 10.0]

**Анализ:**
- Влияние скорости обучения на качество модели
- Поиск оптимального значения learning rate
- Выводы о балансе между скоростью сходимости и стабильностью

### 9. Стохастический градиентный спуск

**Функция `stochastic_gradient_descent`:**
- На каждой итерации выбирается случайный объект
- Градиент вычисляется только по одному объекту
- Обновление весов происходит чаще, но с большим шумом

### 10. Сравнение градиентного и стохастического градиентного спуска

**Функции с историей:**
- `gradient_descent_with_history` - сохраняет MSE на каждой итерации
- `stochastic_gradient_descent_with_history` - аналогично для SGD

**Визуализация:**
- График сходимости обоих методов
- Сравнение плавности траектории
- Анализ финальных значений ошибки

**Выводы:**
- Градиентный спуск: плавная сходимость, стабильный результат
- Стохастический ГС: быстрая сходимость, но с колебаниями

### 11. L2 регуляризация (Ridge)

**Функция `gradient_descent_l2`:**
- Добавление регуляризационного слагаемого к градиенту
- Штраф за большие веса: `λ||w||²`
- Регуляризация не применяется к bias

**Тестирование:**
- Различные значения коэффициента регуляризации λ
- Анализ влияния на MSE и норму весов

### 12. Анализ влияния регуляризации

**20 коэффициентов в диапазоне [0.001, 1000]:**
- Логарифмическая шкала для равномерного покрытия диапазона
- График изменения каждого веса от λ
- График изменения MSE от λ

**Выводы:**
- При увеличении λ веса уменьшаются
- Слишком большая регуляризация может ухудшить качество
- Необходим баланс между переобучением и недообучением

## Требования

- Python 3.7+
- NumPy
- Pandas
- Matplotlib
- Seaborn
- Scikit-learn

## Запуск

1. Установите необходимые зависимости
2. Откройте `lab3.ipynb` в Jupyter Notebook или JupyterLab
3. Выполните все ячейки последовательно

## Результаты

После выполнения всех ячеек будут созданы:
- 6 CSV файлов с различными версиями датасета
- Множество графиков для анализа данных и алгоритмов
- Реализованные функции градиентного спуска с регуляризацией

## Выводы

Лабораторная работа демонстрирует:
- Важность масштабирования данных
- Различия между batch и stochastic градиентным спуском
- Влияние регуляризации на обобщающую способность модели
- Практическую реализацию основных алгоритмов машинного обучения
